"use strict";(self.webpackChunkacmw_jump_prakash=self.webpackChunkacmw_jump_prakash||[]).push([[450],{533:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>m,frontMatter:()=>i,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"documentation/etl-process","title":"Building ETL Pipelines","description":"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing","source":"@site/docs/documentation/etl-process.md","sourceDirName":"documentation","slug":"/documentation/etl-process","permalink":"/ACMW-Jump-Project/docs/documentation/etl-process","draft":false,"unlisted":false,"editUrl":"https://github.com/shreyaa1811/ACMW-Jump-Project/tree/main/docs/documentation/etl-process.md","tags":[],"version":"current","frontMatter":{"title":"Building ETL Pipelines","sidebar_label":"ETL Pipelines","description":"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing"}}');var s=r(4848),a=r(8453);const i={title:"Building ETL Pipelines",sidebar_label:"ETL Pipelines",description:"Understanding and implementing Extract, Transform, Load (ETL) pipelines for data processing"},l="Building ETL Pipelines",o={},d=[{value:"ETL Process Components",id:"etl-process-components",level:2},{value:"1. Extract",id:"1-extract",level:3},{value:"2. Transform",id:"2-transform",level:3},{value:"3. Load",id:"3-load",level:3},{value:"Example ETL Pipeline in Python",id:"example-etl-pipeline-in-python",level:2},{value:"Best Practices for ETL Development",id:"best-practices-for-etl-development",level:2},{value:"Modern ETL Tools",id:"modern-etl-tools",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"building-etl-pipelines",children:"Building ETL Pipelines"})}),"\n",(0,s.jsx)(n.p,{children:"ETL (Extract, Transform, Load) is a fundamental data processing pattern used to collect data from various sources, transform it to meet business needs, and load it into a target destination like a data warehouse."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"ETL Process Diagram",src:r(6393).A+"",width:"945",height:"483"})}),"\n",(0,s.jsx)(n.h2,{id:"etl-process-components",children:"ETL Process Components"}),"\n",(0,s.jsx)(n.h3,{id:"1-extract",children:"1. Extract"}),"\n",(0,s.jsx)(n.p,{children:"The extract phase involves pulling data from source systems. These sources could be:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Relational databases"}),"\n",(0,s.jsx)(n.li,{children:"APIs"}),"\n",(0,s.jsx)(n.li,{children:"Flat files (CSV, JSON)"}),"\n",(0,s.jsx)(n.li,{children:"Web scraping"}),"\n",(0,s.jsx)(n.li,{children:"IoT devices"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"2-transform",children:"2. Transform"}),"\n",(0,s.jsx)(n.p,{children:"The transformation phase involves cleaning, validating, and restructuring the data to prepare it for analysis:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data cleansing"}),"\n",(0,s.jsx)(n.li,{children:"Normalization"}),"\n",(0,s.jsx)(n.li,{children:"Aggregation"}),"\n",(0,s.jsx)(n.li,{children:"Filtering"}),"\n",(0,s.jsx)(n.li,{children:"Type conversion"}),"\n",(0,s.jsx)(n.li,{children:"Key generation"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"3-load",children:"3. Load"}),"\n",(0,s.jsx)(n.p,{children:"The loading phase involves writing the processed data to a destination system:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data warehouses"}),"\n",(0,s.jsx)(n.li,{children:"Data lakes"}),"\n",(0,s.jsx)(n.li,{children:"Databases"}),"\n",(0,s.jsx)(n.li,{children:"Application data stores"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"example-etl-pipeline-in-python",children:"Example ETL Pipeline in Python"}),"\n",(0,s.jsx)(n.p,{children:"Here's a simple ETL pipeline using Python with Pandas for transforming CSV data:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import pandas as pd\r\nimport numpy as np\r\nfrom datetime import datetime\r\n\r\ndef extract():\r\n    \"\"\"Extract data from CSV files\"\"\"\r\n    print(\"Extracting data...\")\r\n    \r\n    # Load sales data\r\n    sales_df = pd.read_csv('sales_data.csv')\r\n    \r\n    # Load customer data\r\n    customer_df = pd.read_csv('customer_data.csv')\r\n    \r\n    return sales_df, customer_df\r\n\r\ndef transform(sales_df, customer_df):\r\n    \"\"\"Transform the extracted data\"\"\"\r\n    print(\"Transforming data...\")\r\n    \r\n    # Clean sales data\r\n    sales_df = sales_df.dropna(subset=['transaction_id', 'product_id'])\r\n    sales_df['sale_date'] = pd.to_datetime(sales_df['sale_date'])\r\n    sales_df['amount'] = sales_df['amount'].astype(float)\r\n    \r\n    # Add calculated fields\r\n    sales_df['month'] = sales_df['sale_date'].dt.month\r\n    sales_df['year'] = sales_df['sale_date'].dt.year\r\n    \r\n    # Join with customer data\r\n    merged_df = pd.merge(\r\n        sales_df,\r\n        customer_df[['customer_id', 'customer_name', 'segment']],\r\n        on='customer_id',\r\n        how='left'\r\n    )\r\n    \r\n    # Calculate aggregations\r\n    summary_df = merged_df.groupby(['year', 'month', 'segment']).agg({\r\n        'amount': ['sum', 'mean', 'count'],\r\n        'transaction_id': pd.Series.nunique\r\n    }).reset_index()\r\n    \r\n    summary_df.columns = ['year', 'month', 'segment', 'total_sales', \r\n                         'average_sale', 'transaction_count', 'unique_transactions']\r\n    \r\n    return merged_df, summary_df\r\n\r\ndef load(merged_df, summary_df):\r\n    \"\"\"Load transformed data to destination\"\"\"\r\n    print(\"Loading data...\")\r\n    \r\n    # Save detailed data to parquet format\r\n    merged_df.to_parquet('processed_sales_data.parquet')\r\n    \r\n    # Save summary data to CSV\r\n    summary_df.to_csv('sales_summary.csv', index=False)\r\n    \r\n    # In a real scenario, you might load to a database\r\n    print(f\"Data successfully loaded. Processed {len(merged_df)} rows.\")\r\n    print(f\"Created summary with {len(summary_df)} rows.\")\r\n\r\ndef run_etl_pipeline():\r\n    \"\"\"Run the full ETL pipeline\"\"\"\r\n    start_time = datetime.now()\r\n    print(f\"Starting ETL pipeline at {start_time}\")\r\n    \r\n    # Extract\r\n    sales_df, customer_df = extract()\r\n    \r\n    # Transform\r\n    merged_df, summary_df = transform(sales_df, customer_df)\r\n    \r\n    # Load\r\n    load(merged_df, summary_df)\r\n    \r\n    end_time = datetime.now()\r\n    print(f\"ETL pipeline completed at {end_time}\")\r\n    print(f\"Total runtime: {end_time - start_time}\")\r\n\r\nif __name__ == \"__main__\":\r\n    run_etl_pipeline()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"best-practices-for-etl-development",children:"Best Practices for ETL Development"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Idempotency"}),": Ensure your pipelines can be run multiple times without creating duplicate data"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Implement robust error handling and logging"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Monitoring"}),": Set up monitoring for your ETL processes with alerts"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Incremental Processing"}),": Process only new or changed data when possible"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Validation"}),": Validate input and output data with schema enforcement"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Version Control"}),": Keep your ETL code in version control"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Documentation"}),": Document data lineage and transformation logic"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"modern-etl-tools",children:"Modern ETL Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache Airflow"}),": Workflow management platform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apache NiFi"}),": Data flow automation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"AWS Glue"}),": Serverless ETL service"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Talend"}),": Open-source integration platform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Informatica PowerCenter"}),": Enterprise data integration"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},6393:(e,n,r)=>{r.d(n,{A:()=>t});const t=r.p+"assets/images/etl-831cfbc4630fa60192ec1ef52897e608.png"},8453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var t=r(6540);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);